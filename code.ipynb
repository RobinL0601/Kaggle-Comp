{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0Jz95-wVjtb"
      },
      "source": [
        "# Deep Learning Clothes Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiUQO0pJ-Evq"
      },
      "source": [
        "This notebook requires Pytorch, torchtext, and timm libraries. Training the models requires at least 6 GB of GPU memory. I trained the models on my laptop, but the code should also run on Google Colab with a GPU setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86tJixQ6CiHk"
      },
      "source": [
        "GPU on my laptop: NVIDIA GeForce RTX 3060"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFC_LMCw_2n6"
      },
      "source": [
        "The code expects the following directory structure:\n",
        "- code.ipynb\n",
        "- models\n",
        "- dataset\n",
        "    - noisy-images\n",
        "        - 3257.jpg\n",
        "        - ...\n",
        "    - train.csv\n",
        "    - test.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT0JAPoiVjtf"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "657z8M3nVjtg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "# pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision import transforms\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "# pytorch image models\n",
        "import timm\n",
        "\n",
        "# ensemble\n",
        "from scipy.stats import mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mcu45AGEVjti"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THpM-mT3Vjti"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gllmDi1RVjti"
      },
      "source": [
        "Preprocess the noisy text description in the train and test dataset. Remove prunctuations and rare words. Appends the categorical data to the front of the noisy text description. Write the cleaned description to new dataframes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qxRtEAJdVjtj"
      },
      "outputs": [],
      "source": [
        "# train data\n",
        "train_data = pd.read_csv('./dataset/train.csv')\n",
        "\n",
        "# words in categorical data\n",
        "common_words = {c.lower() for c in train_data['baseColour']}\n",
        "common_words.update({use.lower() for use in train_data['usage']})\n",
        "common_words.update({s.lower() for s in train_data['season']})\n",
        "common_words = list(common_words)\n",
        "common_words.extend(['women', 'woman', 'womens', 'men', 'man', 'mens'\n",
        "                     'unisex', 'girls', 'girl', 'boys', 'boy'])\n",
        "\n",
        "# rare words in description\n",
        "occurrenecs = dict()\n",
        "for i in range(len(train_data)):\n",
        "    description = train_data.iloc[i]['noisyTextDescription'].lower()\n",
        "    tokens = description.lower().split()\n",
        "    for token in tokens:\n",
        "        if token not in occurrenecs:\n",
        "            occurrenecs[token] = 1\n",
        "        else:\n",
        "            occurrenecs[token] += 1\n",
        "rare_words = [key for key, value in occurrenecs.items() if value <= 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_0yxZ5z1Vjtj"
      },
      "outputs": [],
      "source": [
        "def clean_tokens(text: str, common_words: list, rare_words: list) -> list:\n",
        "    \"\"\"\n",
        "    Removes punctuations, rare words and common words from a single text description.\n",
        "    Splits the text description into a list of tokens.\n",
        "    \n",
        "    :param text: A single noisy text description\n",
        "    :param common_words: A list of words that occur in the categorical data,\n",
        "        will remove them from the noisy text description\n",
        "    :param rare_words: A list of rare words in the noisy text description,\n",
        "        will remove them from the noisy text description\n",
        "    :return: A list of tokens corresponding to a single text description\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # remove rare words and any words that contain r\"[0-9\\/\\=\\*\\\"\\n]\"\n",
        "    new_tokens = []\n",
        "    for token in text.split():\n",
        "        if (token not in rare_words and\n",
        "            not re.search(r\"[0-9\\/\\=\\*\\\"\\n]\", token)):\n",
        "            new_tokens.append(token)\n",
        "    text = \" \".join(new_tokens)\n",
        "    # remove \"'s\"\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    # remove \"\\&\"\n",
        "    text = re.sub(r\"\\&+\", \" \", text)\n",
        "    # remove \"\\-\"\n",
        "    text = re.sub(r\"\\-+\", \" \", text)\n",
        "    # remove \"\\+\"\n",
        "    text = re.sub(r\"\\++\", \" \", text)\n",
        "    # remove words that appear in categorical data\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "        if token not in common_words:\n",
        "            tokens.append(token)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DTj-okbsVjtk"
      },
      "outputs": [],
      "source": [
        "def clean_data(dataframe: pd.DataFrame, common_words: list, rare_words: list) -> tuple:\n",
        "    \"\"\"\n",
        "    Cleans the noisy text description column in a dataframe and\n",
        "    puts the cleaned desciption in a new dataframe.\n",
        "    Removes punctuations, rare words and common words from the noisy text description.\n",
        "    Appends categorical data to the front of text description.\n",
        "\n",
        "    :param dataframe: A single dataframe that contains the columns\n",
        "        'gender', 'season', 'usage', 'baseColour', and 'noisyTextDescription'\n",
        "    :param common_words: A list of words that occur in the categorical data,\n",
        "        will remove them from the noisy text description\n",
        "    :param rare_words: A list of rare words in the noisy text description,\n",
        "        will remove them from the noisy text description\n",
        "    :return: Tuple of the following:\n",
        "    * A new dataframe with a column 'description' added to the original dataframe\n",
        "    * Maximum number of tokens in a single entry under the column 'description'\n",
        "    \"\"\"\n",
        "    description = []\n",
        "    max_seq = 0\n",
        "    for i in range(len(dataframe)):\n",
        "        gender = dataframe.iloc[i]['gender'].lower()\n",
        "        season = dataframe.iloc[i]['season'].lower()\n",
        "        usage = dataframe.iloc[i]['usage'].lower()\n",
        "        colour = dataframe.iloc[i]['baseColour'].lower()\n",
        "        text = dataframe.iloc[i]['noisyTextDescription'].lower()\n",
        "        new_tokens = clean_tokens(text, common_words, rare_words)\n",
        "        # append words in categorical data\n",
        "        tokens = [gender, season, usage, colour]\n",
        "        tokens.extend(new_tokens)\n",
        "        max_seq = max(max_seq, len(tokens))\n",
        "        text = \" \".join(tokens)\n",
        "        description.append(text)\n",
        "    dataframe['description'] = description\n",
        "    return dataframe, max_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data max sequence: 5\n",
            "Test data max sequence: 9\n"
          ]
        }
      ],
      "source": [
        "# clean train data\n",
        "train_data, max_seq= clean_data(train_data, common_words, rare_words)\n",
        "print('Train data max sequence:', max_seq)\n",
        "\n",
        "# clean test data\n",
        "test_data = pd.read_csv('./dataset/test.csv')\n",
        "test_data, max_seq = clean_data(test_data, common_words, rare_words)\n",
        "print('Test data max sequence:', max_seq)\n",
        "\n",
        "# save\n",
        "train_ratio = 0.8\n",
        "train_len = int(len(train_data) * train_ratio)\n",
        "train_data.iloc[:train_len].to_csv('./dataset/train_cleaned.csv', index=False)\n",
        "train_data.iloc[train_len:].to_csv('./dataset/val_cleaned.csv', index=False)\n",
        "test_data.to_csv('./dataset/test_cleaned.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwnsnNqkVjtl"
      },
      "source": [
        "## Build vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RRur3SwhVjtl"
      },
      "outputs": [],
      "source": [
        "def yield_description_tokens(dataframe: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates lists of tokens for each description entry in a dataframe.\n",
        "\n",
        "    :param dataframe: Training data that contains column 'description'\n",
        "    \"\"\"\n",
        "    for i in range(len(dataframe)):\n",
        "        description = dataframe.iloc[i]['description']\n",
        "        yield description.split()\n",
        "\n",
        "def yield_category_tokens(dataframe: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Generates category for each target entry in a dataframe.\n",
        "\n",
        "    :param dataframe: Training data that contains column 'target'\n",
        "    \"\"\"\n",
        "    for i in range(len(dataframe)):\n",
        "        target = dataframe.iloc[i]['category']\n",
        "        yield [target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQOcOJcQVjtm"
      },
      "source": [
        "From training data, build vocabulary, i.e. mapping from tokens to indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "rwXzeyh3Vjtm"
      },
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "# builds vocabulary for text description from training data\n",
        "description_vocab = build_vocab_from_iterator(yield_description_tokens(train_data))\n",
        "# builds vocubulary\n",
        "category_vocab = build_vocab_from_iterator(yield_category_tokens(train_data))\n",
        "### run this on Colab, for new version of torchtext\n",
        "# # builds vocabulary for text description from training data\n",
        "# description_vocab = build_vocab_from_iterator(yield_description_tokens(train_data), specials=['<unk>', '<pad>'])\n",
        "# description_vocab.set_default_index(0)\n",
        "# # builds vocubulary\n",
        "# category_vocab = build_vocab_from_iterator(yield_category_tokens(train_data), specials=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDgForvSVjtm",
        "outputId": "ae897b1b-a078-4b6e-de8c-3b61fd0433d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The first few tokens in the text description vocabulary:\n",
            "['casual', 'summer', 'men', 'blue', 'women', 'watch', 'black', 'fall', 'formal', 'green']\n",
            "The number of tokens in the text description vocabulary: 24\n",
            "\n",
            "All tokens in the target category vocabulary:\n",
            "['Topwear', 'Bottomwear', 'Sandal', 'Shoes', 'Fragrance', 'Innerwear', 'Loungewear and Nightwear', 'Ties', 'Watches']\n",
            "The number of tokens in the target category vocabulary: 9\n"
          ]
        }
      ],
      "source": [
        "print('The first few tokens in the text description vocabulary:')\n",
        "print(description_vocab.get_itos()[:10])\n",
        "print('The number of tokens in the text description vocabulary:', len(description_vocab.get_itos()))\n",
        "print('\\nAll tokens in the target category vocabulary:')\n",
        "print(category_vocab.get_itos())\n",
        "print('The number of tokens in the target category vocabulary:', len(category_vocab.get_itos()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hx6BYiGVjtm"
      },
      "source": [
        "## Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6ycbIbJiVjtn"
      },
      "outputs": [],
      "source": [
        "# custom dataset, loads images, text descriptions, and categories\n",
        "# apply resize, random horizontal flip and random noise to images\n",
        "\n",
        "class ImageTextDataset(Dataset):\n",
        "    def __init__(self, dataframe_path: str, image_root: str,\n",
        "                 text_vocab: Vocab, category_vocab: Vocab,\n",
        "                 max_seq: int, noise_prob:float, transform, train=True):\n",
        "        self.dataframe = pd.read_csv(dataframe_path)\n",
        "        self.text_vocab = text_vocab\n",
        "        self.texts = torch.ones(len(self.dataframe), max_seq, dtype=torch.long)\n",
        "        self.image_root = image_root\n",
        "        self.transform_image = transform\n",
        "        self.category_vocab = category_vocab\n",
        "        self.max_seq = max_seq\n",
        "        self.noise = None\n",
        "        self.noise_prob = noise_prob\n",
        "        self.train = train\n",
        "        for i in range(len(self.dataframe)):\n",
        "            # text description\n",
        "            description = self.dataframe.iloc[i]['description']\n",
        "            tokens = description.split()\n",
        "            for j, token in enumerate(tokens):\n",
        "                self.texts[i][j] = text_vocab[token]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # image\n",
        "        image_idx = int(self.dataframe.iloc[idx]['id'])\n",
        "        image_path = f'{self.image_root}/{image_idx}.jpg'\n",
        "        image = Image.open(image_path)\n",
        "        image = self.transform_image(image)\n",
        "        self.noise = image\n",
        "        prob = np.random.uniform(0, 1, 1)\n",
        "        if prob < self.noise_prob and self.noise is not None:\n",
        "            image_transformed = 0.95 * image + 0.05 * self.noise\n",
        "        else:\n",
        "            image_transformed = image\n",
        "        # text description\n",
        "        text = self.texts[idx]\n",
        "        if not self.train:\n",
        "            return image_transformed, text\n",
        "        # category\n",
        "        category = self.dataframe.iloc[idx]['category']\n",
        "        category = self.category_vocab[category]\n",
        "\n",
        "        return image_transformed, text, category"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0t0J3CmVjtn"
      },
      "source": [
        "## Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LaUqz-YVjtn"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrxkFv-G9ff7"
      },
      "source": [
        "Transformer model for text data, only uses encoding layers but does not use decoding layers. Added a fully connected classification layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X4-g5q_LVjtn"
      },
      "outputs": [],
      "source": [
        "# transformer positional embedding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model: int, max_length: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_length, d_model)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, : x.size(1), :]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1hmtGO68Vjtn"
      },
      "outputs": [],
      "source": [
        "# Transformer text classifier\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size: int,\n",
        "        d_model: int,\n",
        "        output_size: int,\n",
        "        max_length: int,\n",
        "        nhead: int = 8,\n",
        "        dim_feedforward: int = 512,\n",
        "        num_layers: int = 6,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert d_model % nhead == 0, \"nheads must divide evenly into d_model\"\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(\n",
        "            d_model=d_model, max_length=max_length, dropout=dropout\n",
        "        )\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=num_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(d_model, output_size)\n",
        "        self.d_model = d_model\n",
        "        self.num_features = d_model\n",
        "        self.num_classes = output_size\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def forward_features(self, x):\n",
        "        x = self.embedding(x) * math.sqrt(self.d_model)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWnsycuWVjto"
      },
      "source": [
        "### Image Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_lmn8RlVjto"
      },
      "source": [
        "A wrapper class for the image models (Resnet, Efficientnet, etc.) in timm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "u0Rxz6_UzdCk"
      },
      "outputs": [],
      "source": [
        "class ImageModel(nn.Module):\n",
        "    def __init__(self, backbone, output_size=27):\n",
        "        super(ImageModel, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.fc1 = nn.Linear(1000, 256)\n",
        "        self.fc2 = nn.Linear(256, output_size)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.backbone(x)\n",
        "        output = self.relu(self.fc1(output))\n",
        "        return self.fc2(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQRGX53nVjto"
      },
      "source": [
        "### Image Text Models"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zkMYrZN6Vjto"
      },
      "source": [
        "Combine image models with text models. Concatenate the embeddings of image and text models. Add a classifier to the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "qnl7Xa8MVjto"
      },
      "outputs": [],
      "source": [
        "# image text model\n",
        "class ImageTextClassifier(nn.Module):\n",
        "    def __init__(self, image_model, text_model, hidden_size: int,\n",
        "                 num_classes: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.image_model = image_model\n",
        "        self.text_model = text_model\n",
        "        self.num_features = image_model.num_features + text_model.num_features\n",
        "        self.fc1 = nn.Linear(self.num_features, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_size, 128)\n",
        "        self.fc3 = nn.Linear(128, num_classes)\n",
        "\n",
        "    def forward(self, image, text):\n",
        "        x1 = self.image_model.forward_features(image)\n",
        "        x1 = self.image_model.forward_head(x1, pre_logits=True).flatten(start_dim=1)\n",
        "        x2 = self.text_model.forward_features(text)\n",
        "        x = torch.concat((x1, x2), dim=1)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHOoy6PfVjto"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JxRUqwbWVjto"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloaders, criterion, optimizer, n_epoch, checkpoint_path,\n",
        "          use_image=True, use_text=True):\n",
        "    \"\"\"\n",
        "    Train loop. Performs both training and validation for each epoch.\n",
        "    Saves the best model according to validation loss.\n",
        "\n",
        "    :param model: Pytorch model\n",
        "    :param dataloaders: A dictionary of two dataloaders that load\n",
        "        image, text, and target category;\n",
        "        training dataloader is at key 'Train',\n",
        "        validation dataloader is at key 'Validation'\n",
        "    :param criterion: Classification loss function\n",
        "    :param n_epoch: Number of epochs\n",
        "    :param checkpoint_path: Output checkpoint path including filename\n",
        "    :param use_image: Whether to input image to the model in the forward step\n",
        "    :param use_text: Whether to input text description to the model in the forward step\n",
        "    :return: Tuple of the following\n",
        "    * A list of train accuracies\n",
        "    * A list of validation accuracies\n",
        "    \"\"\"\n",
        "    # best validation accuracy over all epochs\n",
        "    best_accuracy = 0.0\n",
        "    # train/validation accuracy for each epoch\n",
        "    accuracy_dict = {'Train': [], 'Validation': []}\n",
        "    # Each epoch consists of train and validation\n",
        "    phases = ['Train', 'Validation']\n",
        "    for epoch in range(n_epoch):\n",
        "        print('-'*10)\n",
        "        print(f'Epoch {epoch + 1}/{n_epoch}:')\n",
        "        for phase in phases:\n",
        "            if phase == 'Train':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            running_loss = 0.0\n",
        "            running_correct = 0\n",
        "            running_total = 0\n",
        "            for image, text, target in dataloaders[phase]:\n",
        "                image, text, target = image.to(device), text.to(device), target.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                if use_image and use_text: # image text model\n",
        "                    output = model(image, text)\n",
        "                elif use_image:            # image model\n",
        "                    output = model(image)\n",
        "                else:                      # text model\n",
        "                    output = model(text)\n",
        "                loss = criterion(output, target)\n",
        "                if phase == 'Train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                preds = torch.argmax(output, dim=1)\n",
        "                running_loss += loss.item()\n",
        "                running_correct += (preds == target).sum().item()\n",
        "                running_total += target.size(0)\n",
        "            epoch_loss = running_loss / len(dataloaders[phase])\n",
        "            epoch_accuracy = running_correct / running_total\n",
        "            accuracy_dict[phase].append(epoch_accuracy)\n",
        "            print(f'{phase} loss:', round(epoch_loss, 4),\n",
        "                  f'\\t{phase} accuracy:', round(epoch_accuracy, 4))\n",
        "            if phase == 'Validation' and epoch_accuracy > best_accuracy:\n",
        "                print('Validation accuracy increases from', round(best_accuracy, 4),\n",
        "                      'to', round(epoch_accuracy, 4))\n",
        "                best_accuracy = epoch_accuracy\n",
        "                # save best model according to validation loss\n",
        "                torch.save(\n",
        "                    {'epoch': epoch,\n",
        "                     'model_state_dict': model.state_dict(),\n",
        "                     'optimizer_state_dict': optimizer.state_dict(),\n",
        "                     'loss': epoch_loss},\n",
        "                    checkpoint_path\n",
        "                )\n",
        "    return accuracy_dict['Train'], accuracy_dict['Validation']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tz55Zqi69ff8"
      },
      "source": [
        "## Inference Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "x4b82j2z9ff9"
      },
      "outputs": [],
      "source": [
        "def predict(model, dataloader, use_image=True, use_text=True, val=True):\n",
        "    \"\"\"\n",
        "    Inference loop. Returns the predicted category indices from a single model.\n",
        "    \n",
        "    :param model: Pytorch model.\n",
        "    :param dataloader: A validation or test dataloader.\n",
        "    :param use_image: Whether to input image to the model in the forward step.\n",
        "    :param use_text: Whether to input text description to the model in the forward step.\n",
        "    :param val: Whether to compare predictions with targets and print accuracy.\n",
        "    :return: A numpy array of predicted category indices.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    pred_list = []\n",
        "    for batch in dataloader:\n",
        "        image, text = batch[0].to(device), batch[1].to(device)\n",
        "        if use_image and use_text:\n",
        "            output = model(image, text)\n",
        "        elif use_image:\n",
        "            output = model(image)\n",
        "        else:\n",
        "            output = model(text)\n",
        "        preds = torch.argmax(output, dim=1)\n",
        "        preds = preds.data.cpu().numpy()\n",
        "        if val:\n",
        "            target = batch[2].data.cpu().numpy()\n",
        "            running_correct += (preds == target).sum()\n",
        "            running_total += target.shape[0]\n",
        "        pred_list.append(preds)\n",
        "    pred_list = np.concatenate(pred_list)\n",
        "    if val:\n",
        "        accuracy = running_correct / running_total\n",
        "        print('Accuracy:', round(accuracy, 4))\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hnh4DX4K9ff9"
      },
      "outputs": [],
      "source": [
        "def predict_voting(models, dataloader, use_image=True, use_text=True,\n",
        "                   val=False, weights=None):\n",
        "    \"\"\"\n",
        "    Inference loop. Perform voting with a number of base models.\n",
        "    Weights are optional. If weights are given, then the predictions are \n",
        "    the categories with the maximum weighted sum of logits from base models.\n",
        "    Otherwise, the predictions are the mode of the categories predicted by base models.\n",
        "    \n",
        "    :param models: A list of Pytorch models.\n",
        "    :param dataloader: A validation or test dataloader.\n",
        "    :param use_image: Whether to input image to the model in the forward step.\n",
        "    :param use_text: Whether to input text description to the model in the forward step.\n",
        "    :param val: Whether to compare predictions with targets and print accuracy.\n",
        "    :param weights: Optional list of weights applied to each model when voting.\n",
        "    :return: A numpy array of predicted category indices.\n",
        "    \"\"\"\n",
        "    for model in models:\n",
        "        model.eval()\n",
        "    running_correct = 0\n",
        "    running_total = 0\n",
        "    pred_list = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            image, text = batch[0].to(device), batch[1].to(device)\n",
        "            preds = []\n",
        "            if weights is None:\n",
        "                for i, model in enumerate(models):\n",
        "                    if use_image and use_text:\n",
        "                        output = model(image, text)\n",
        "                    elif use_image:\n",
        "                        output = model(image)\n",
        "                    else:\n",
        "                        output = model(text)\n",
        "                    pred = torch.argmax(output, dim=1)\n",
        "                    preds.append(pred.data.cpu())\n",
        "                preds = torch.stack(preds)\n",
        "                preds = mode(preds, axis=0, keepdims=False)[0]\n",
        "            else:\n",
        "                for i, model in enumerate(models):\n",
        "                    if use_image and use_text:\n",
        "                        output = model(image, text)\n",
        "                    elif use_image:\n",
        "                        output = model(image)\n",
        "                    else:\n",
        "                        output = model(text)\n",
        "                    output = output * weights[i]\n",
        "                    preds.append(output.data.cpu().numpy())\n",
        "                preds = np.stack(preds)\n",
        "                preds = np.argmax(np.sum(preds, axis=0), axis=1)\n",
        "            if val:\n",
        "                target = batch[2].data.cpu().numpy()\n",
        "                running_correct += (preds == target).sum()\n",
        "                running_total += target.shape[0]\n",
        "            pred_list.append(preds)\n",
        "\n",
        "        pred_list = np.concatenate(pred_list)\n",
        "        if val:\n",
        "            accuracy = running_correct / running_total\n",
        "            print('Accuracy:', round(accuracy, 4))\n",
        "    return pred_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MpaPOz8m9ff9"
      },
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "def index_to_category(pred_list, category_vocab):\n",
        "    \"\"\"\n",
        "    Converts category indices to category names.\n",
        "\n",
        "    :param pred_list: A list or numpy array of predicted category indices.\n",
        "    :param category_vocab: A torchtext Vocab object containing all tokens.\n",
        "    :return: A list of category names.\n",
        "    \"\"\"\n",
        "    categories = category_vocab.itos\n",
        "    pred_list = [categories[int(idx)] for idx in pred_list]\n",
        "    return pred_list\n",
        "\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# def index_to_category(pred_list, category_vocab):\n",
        "#     categories = category_vocab.get_itos()\n",
        "#     pred_list = [categories[int(idx)] for idx in pred_list]\n",
        "#     return pred_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlioNbppVjtp"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Jp2lLpL7Vjtp"
      },
      "outputs": [],
      "source": [
        "# configure dataset\n",
        "train_dataframe_path = './dataset/train_cleaned.csv'\n",
        "val_dataframe_path = './dataset/val_cleaned.csv'\n",
        "image_root = './dataset/noisy-images'\n",
        "max_seq = 16\n",
        "batch_size = 48\n",
        "train_noise_prob = 0.5\n",
        "val_noise_prob = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Q1zNY8f8Vjtp"
      },
      "outputs": [],
      "source": [
        "train_transform = transforms.Compose([transforms.Resize(90),\n",
        "                                      transforms.RandomResizedCrop(80), \n",
        "                                      transforms.RandomHorizontalFlip(), \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.8193, 0.8041, 0.7969), (0.2224, 0.2341, 0.2369))])\n",
        "train_dataset = ImageTextDataset(train_dataframe_path, image_root,\n",
        "                           text_vocab=description_vocab,\n",
        "                           category_vocab=category_vocab,\n",
        "                           max_seq=max_seq, noise_prob=train_noise_prob,\n",
        "                           transform=train_transform, train=True)\n",
        "val_dataset = ImageTextDataset(val_dataframe_path, image_root,\n",
        "                           text_vocab=description_vocab,\n",
        "                           category_vocab=category_vocab,\n",
        "                           max_seq=max_seq, noise_prob=val_noise_prob,\n",
        "                           transform=train_transform, train=True)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, drop_last=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
        ")\n",
        "\n",
        "train_dataloaders = {'Train': train_dataloader, 'Validation': val_dataloader}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZD1rPPhyVjtp"
      },
      "outputs": [],
      "source": [
        "test_dataframe_path = './dataset/test_cleaned.csv'\n",
        "image_root = './dataset/noisy-images/noisy-images'\n",
        "max_seq = 16\n",
        "batch_size = 48\n",
        "test_noise_prob = 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "66IohvviVjtp"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "test_transform = transforms.Compose([transforms.Resize(90),\n",
        "                                     transforms.CenterCrop(80),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize((0.8193, 0.8041, 0.7969), (0.2224, 0.2341, 0.2369))])\n",
        "test_dataset = ImageTextDataset(test_dataframe_path, image_root,\n",
        "                                text_vocab=description_vocab,\n",
        "                                category_vocab=category_vocab,\n",
        "                                max_seq=max_seq, noise_prob=test_noise_prob,\n",
        "                                transform=test_transform, train=False)\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset, batch_size=batch_size, shuffle=False, drop_last=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjYOFfmTVjtp",
        "outputId": "28397b3c-863b-4f9d-eccb-45196012467e"
      },
      "outputs": [],
      "source": [
        "# sample train data\n",
        "image, text, target = next(iter(train_dataloaders['Train']))\n",
        "print(image[0])\n",
        "print('Image shape:', image.shape)\n",
        "print(text[0])\n",
        "print('Text shape:', text.shape)\n",
        "print('Target:', target[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOf4bCWTVjtp"
      },
      "outputs": [],
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7U7ravNa9ff-"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcXrJzoF9ff-"
      },
      "source": [
        "### Training Resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hjLRiunVjtq"
      },
      "outputs": [],
      "source": [
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "optimizer = torch.optim.SGD(resnet34.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/resnet34.pth')\n",
        "# resnet34.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51O6JwsXVjtq"
      },
      "outputs": [],
      "source": [
        "# train resnet34, takes an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/resnet34.pth'\n",
        "resnet34_train_accuracy, resnet34_val_accuracy = train(\n",
        "    resnet34, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKgyzfKZ9ff_"
      },
      "source": [
        "### Training Efficientnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp5oujEQ9ff_"
      },
      "outputs": [],
      "source": [
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "optimizer = torch.optim.SGD(efficientnet.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/efficientnetv2.pth')\n",
        "# efficientnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmhDXbUC9ff_",
        "outputId": "554fc297-f052-43cf-8645-660a92a5377b"
      },
      "outputs": [],
      "source": [
        "# train efficientnet v2, takes an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/efficientnetv2.pth'\n",
        "efficientnet_train_accuracy, efficientnet_val_accuracy = train(\n",
        "    efficientnet, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4L5zA3u69ff_"
      },
      "source": [
        "### Training Convnext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiYQkRGD9ff_"
      },
      "outputs": [],
      "source": [
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "optimizer = torch.optim.SGD(convnext.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/convnext_small.pth')\n",
        "# convnext.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W431UNPk9ff_",
        "outputId": "7aae56b4-ede1-4d5f-f522-eafe3590ed70"
      },
      "outputs": [],
      "source": [
        "# train convnext small, takes over an hour\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/convnext_small.pth'\n",
        "convnext_train_accuracy, convnext_val_accuracy = train(\n",
        "    convnext, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LH_pc1XF9ff_"
      },
      "source": [
        "### Training Text Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "input_size = len(description_vocab.itos)\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# input_size = len(description_vocab.get_itos())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfVOdzX59ff_"
      },
      "outputs": [],
      "source": [
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "optimizer = torch.optim.SGD(transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/transformer.pth')\n",
        "# transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aU4rW7xp9fgA",
        "outputId": "3ea9f753-2fd3-4fdc-d36c-c61ce0a6528c"
      },
      "outputs": [],
      "source": [
        "# train transformer, takes an hour\n",
        "n_epoch = 80  # can modify\n",
        "checkpoint_path = './models/transformer.pth'\n",
        "transformer_train_accuracy, transformer_val_accuracy = train(\n",
        "    transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=False, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKvgyecc9fgA"
      },
      "source": [
        "### Training Resnet + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE6li6qe9fgA",
        "outputId": "ec8d852c-9e67-46e9-b5a8-72f22f78d3b4"
      },
      "outputs": [],
      "source": [
        "# create resnet34 and load checkpoint, no grad\n",
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "checkpoint = torch.load('./models/resnet34.pth')\n",
        "resnet34.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in resnet34.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create resnet34 + transformer model and load checkpoint\n",
        "resnet34_transformer = ImageTextClassifier(resnet34, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "resnet34_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(resnet34_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/resnet34_transformer.pth')\n",
        "# resnet34_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lFom8CB9fgA",
        "outputId": "4c785852-b77a-4e79-9dc1-1b5cde8f8de1"
      },
      "outputs": [],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/resnet34_transformer.pth'\n",
        "resnet34_transformer_train_accuracy, resnet34_transformer_val_accuracy = train(\n",
        "    resnet34_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHrq7cu49fgA"
      },
      "source": [
        "### Training Efficientnet + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fSnXc7q9fgA"
      },
      "outputs": [],
      "source": [
        "# create efficientnet v2 and load checkpoint, no grad\n",
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "checkpoint = torch.load('./models/efficientnetv2.pth')\n",
        "efficientnet.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in efficientnet.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create efficientnet v2 + transformer and load checkpoint\n",
        "efficientnet_transformer = ImageTextClassifier(efficientnet, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "efficientnet_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(efficientnet_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/efficientnetv2_transformer.pth')\n",
        "# efficientnet_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZnYVADx9fgA",
        "outputId": "70ede617-a581-4dc2-aac5-82a0c8c3cd0d"
      },
      "outputs": [],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/efficientnetv2_transformer.pth'\n",
        "efficientnet_transformer_train_accuracy, efficientnet_transformer_val_accuracy = train(\n",
        "    efficientnet_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtZoNyTT9fgB"
      },
      "source": [
        "### Training Convnext + Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwzJhiiS9fgB",
        "outputId": "93f5923c-4c3e-41b6-81a2-6422a06f2b31"
      },
      "outputs": [],
      "source": [
        "# create convnext small and load checkpoint, no grad\n",
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "checkpoint = torch.load('./models/convnext_small.pth')\n",
        "convnext.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in convnext.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create transformer and load checkpoint, no grad\n",
        "# input_size defined before\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "checkpoint = torch.load('./models/transformer.pth')\n",
        "transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "for param in transformer.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# create convnext small + transformer model and load checkpoint\n",
        "convnext_transformer = ImageTextClassifier(convnext, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "convnext_transformer.to(device)\n",
        "optimizer = torch.optim.SGD(convnext_transformer.parameters(), lr=3e-4, momentum=0.9)\n",
        "\n",
        "# checkpoint = torch.load('./models/convnext_small_transformer.pth')\n",
        "# convnext_transformer.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA77bLai9fgB",
        "outputId": "58cfef7f-6bae-441f-bd34-68d93dc6bd1e"
      },
      "outputs": [],
      "source": [
        "# train, takes 30 min\n",
        "n_epoch = 60  # can modify\n",
        "checkpoint_path = './models/convnext_small_transformer.pth'\n",
        "convnext_transformer_train_accuracy, convnext_transformer_val_accuracy = train(\n",
        "    convnext_transformer, train_dataloaders, criterion, optimizer, n_epoch=n_epoch,\n",
        "    checkpoint_path=checkpoint_path, use_image=True, use_text=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbDOhrFT9fgB"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP-rV_259fgB"
      },
      "source": [
        "Voting ensemble using the three base models: Resnet34 + Transformer, Efficient V2 + Ensemble, and Convnext Small + Ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### run this locally, for older version of torchtext\n",
        "input_size = len(description_vocab.itos)\n",
        "\n",
        "### run this on Colab, for new version of torchtext\n",
        "# input_size = len(description_vocab.get_itos())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k4Wb3mN9fgB",
        "outputId": "ac461793-7196-43da-b6a5-0d8f829f621a"
      },
      "outputs": [],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "resnet34 = timm.create_model('resnet34', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "resnet34 = resnet34.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create resnet34 + transformer model and load checkpoint\n",
        "resnet34_transformer = ImageTextClassifier(resnet34, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "resnet34_transformer.to(device)\n",
        "checkpoint = torch.load('./models/resnet34_transformer.pth')\n",
        "resnet34_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlHWj6yM9fgB",
        "outputId": "d324950e-190f-4fc7-890a-87845eb30aea"
      },
      "outputs": [],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "efficientnet = timm.create_model('efficientnetv2_rw_t', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "efficientnet = efficientnet.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create efficientnet v2 + transformer model and load checkpoint\n",
        "efficientnet_transformer = ImageTextClassifier(efficientnet, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "efficientnet_transformer.to(device)\n",
        "checkpoint = torch.load('./models/efficientnetv2_transformer.pth')\n",
        "efficientnet_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prdyCql39fgC",
        "outputId": "599626d0-c29c-4b3e-848c-16fb0b5c74d7"
      },
      "outputs": [],
      "source": [
        "# create resnet34 and load checkpoint\n",
        "convnext = timm.create_model('convnext_small_in22k', num_classes=27, pretrained=True, drop_rate=0.7)\n",
        "convnext = convnext.to(device)\n",
        "\n",
        "# create transformer and load checkpoint\n",
        "# input_size defined above\n",
        "d_model = 512\n",
        "output_size = 27\n",
        "max_length = 16\n",
        "nhead = 8\n",
        "dim_feedforward = 512\n",
        "num_layers = 6\n",
        "dropout = 0.4\n",
        "\n",
        "transformer = Transformer(\n",
        "    input_size=input_size,\n",
        "    d_model=d_model,\n",
        "    output_size=output_size,\n",
        "    max_length=max_length,\n",
        "    nhead=nhead,\n",
        "    dim_feedforward=dim_feedforward,\n",
        "    num_layers=num_layers,\n",
        "    dropout=dropout)\n",
        "transformer.to(device)\n",
        "\n",
        "# create convnext small + transformer model and load checkpoint\n",
        "convnext_transformer = ImageTextClassifier(convnext, transformer, hidden_size=256, num_classes=27, dropout=0.3)\n",
        "convnext_transformer.to(device)\n",
        "checkpoint = torch.load('./models/convnext_small_transformer.pth')\n",
        "convnext_transformer.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qz2xFGiY9fgC",
        "outputId": "b1081b3c-fa97-4db1-b266-b81b266103ed"
      },
      "outputs": [],
      "source": [
        "# base models validation\n",
        "print('Validation started...')\n",
        "print('Resnet34 + Transformer ', end='')\n",
        "pred_list = predict(resnet34_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Efficient V2 + Transformer ', end='')\n",
        "pred_list = predict(efficientnet_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Convnext Small + Transformer ', end='')\n",
        "pred_list = predict(convnext_transformer, val_dataloader, use_image=True, use_text=True, val=True)\n",
        "print('Validation completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPVBwtWP9fgC",
        "outputId": "5c35c0f5-aad6-479f-dcc9-dcac9b773114"
      },
      "outputs": [],
      "source": [
        "# weighted voting ensemble, use validation accuracies as weights\n",
        "models = [resnet34_transformer, efficientnet_transformer, convnext_transformer]\n",
        "weights = [0.9036, 0.9008, 0.9115]\n",
        "\n",
        "# weighted voting ensemble validation\n",
        "print('Validation started...')\n",
        "print('Voting ', end='')\n",
        "pred_list = predict_voting(models, val_dataloader, use_image=True, use_text=True, val=True, weights=weights)\n",
        "print('Validation completed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ja6xwYHz9fgC",
        "outputId": "04f3ad75-75e5-4e7e-f5a6-eb780ea823c4"
      },
      "outputs": [],
      "source": [
        "# weighted voting ensemble testing\n",
        "print('Inference started...')\n",
        "pred_list = predict_voting(models, test_dataloader, use_image=True, use_text=True, val=False, weights=weights)\n",
        "print('Inference completed')\n",
        "\n",
        "print('Converting category indices to category names...')\n",
        "pred_list = index_to_category(pred_list, category_vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP90jD9G9fgC"
      },
      "outputs": [],
      "source": [
        "# prediction file to be submitted\n",
        "test_data = pd.read_csv('./dataset/test_cleaned.csv')\n",
        "pred_data = test_data[['id']]\n",
        "pred_data.insert(1, 'category', pred_list)\n",
        "pred_data.to_csv('./dataset/predict.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "cs480",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
